{"cells":[{"cell_type":"code","execution_count":1,"id":"c9ec9784","metadata":{},"outputs":[],"source":["from pyspark import SparkContext\n","from pyspark.streaming import StreamingContext"]},{"cell_type":"code","execution_count":2,"id":"67fc85b8","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","22/05/03 19:12:27 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","22/05/03 19:12:27 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","22/05/03 19:12:27 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/05/03 19:12:27 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"]}],"source":["sc = SparkContext(appName=\"p2t2\")"]},{"cell_type":"code","execution_count":3,"id":"402a5586","metadata":{},"outputs":[],"source":["ssc = StreamingContext(sc, 10)"]},{"cell_type":"code","execution_count":4,"id":"6f7ad569","metadata":{},"outputs":[],"source":["pagestream = ssc.textFileStream(\"gs://genishk-bucket1/testq\")\n","pagestream.saveAsTextFiles(\"gs://genishk-bucket1/testq3/test_emit.csv\")"]},{"cell_type":"code","execution_count":5,"id":"ce674311","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/05/03 19:13:24 WARN org.apache.spark.streaming.StreamingContext: Dynamic Allocation is enabled for this application. Enabling Dynamic allocation for Spark Streaming applications can cause data loss if Write Ahead Log is not enabled for non-replayable sources. See the programming guide for details on how to enable the Write Ahead Log.\n","22/05/03 19:13:30 WARN org.apache.spark.streaming.dstream.FileInputDStream: Error finding new files under gs://genishk-bucket1/testq\n","java.io.IOException: Error accessing gs://genishk-bucket1/testq\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)\n","\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:281)\n","\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\n","\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2069)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1098)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1059)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:195)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:146)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n","\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n","\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n","\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n","\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n","\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n","\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n","\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 42 more\n","22/05/03 19:13:30 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651605210000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 262, in saveAsTextFile\n","    rdd.saveAsTextFile(path)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1826, in saveAsTextFile\n","    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o69.saveAsTextFile.\n",": java.io.IOException: Error accessing gs://genishk-bucket1/testq3/test_emit.csv-1651605210000\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1691)\n","\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:130)\n","\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:298)\n","\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:551)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:550)\n","\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq3%2Ftest_emit.csv-1651605210000?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 58 more\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/03 19:13:40 WARN org.apache.spark.streaming.dstream.FileInputDStream: Error finding new files under gs://genishk-bucket1/testq\n","java.io.IOException: Error accessing gs://genishk-bucket1/testq\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)\n","\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:281)\n","\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\n","\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2069)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1098)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1059)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:195)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:146)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n","\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n","\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n","\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n","\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n","\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n","\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n","\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 42 more\n","22/05/03 19:13:41 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651605220000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 262, in saveAsTextFile\n","    rdd.saveAsTextFile(path)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1826, in saveAsTextFile\n","    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o84.saveAsTextFile.\n",": java.io.IOException: Error accessing gs://genishk-bucket1/testq3/test_emit.csv-1651605220000\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1691)\n","\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:130)\n","\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:298)\n","\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:551)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:550)\n","\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq3%2Ftest_emit.csv-1651605220000?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 58 more\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/03 19:13:50 WARN org.apache.spark.streaming.dstream.FileInputDStream: Error finding new files under gs://genishk-bucket1/testq\n","java.io.IOException: Error accessing gs://genishk-bucket1/testq\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)\n","\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:281)\n","\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\n","\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2069)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1098)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1059)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:195)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:146)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n","\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n","\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n","\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n","\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n","\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n","\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n","\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 42 more\n","22/05/03 19:13:50 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651605230000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 262, in saveAsTextFile\n","    rdd.saveAsTextFile(path)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1826, in saveAsTextFile\n","    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o99.saveAsTextFile.\n",": java.io.IOException: Error accessing gs://genishk-bucket1/testq3/test_emit.csv-1651605230000\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1691)\n","\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:130)\n","\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:298)\n","\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:551)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:550)\n","\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq3%2Ftest_emit.csv-1651605230000?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 58 more\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/03 19:14:00 WARN org.apache.spark.streaming.dstream.FileInputDStream: Error finding new files under gs://genishk-bucket1/testq\n","java.io.IOException: Error accessing gs://genishk-bucket1/testq\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)\n","\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:281)\n","\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\n","\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2069)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1098)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1059)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:195)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:146)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n","\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n","\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n","\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n","\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n","\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n","\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n","\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 42 more\n","22/05/03 19:14:00 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651605240000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 262, in saveAsTextFile\n","    rdd.saveAsTextFile(path)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1826, in saveAsTextFile\n","    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o114.saveAsTextFile.\n",": java.io.IOException: Error accessing gs://genishk-bucket1/testq3/test_emit.csv-1651605240000\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1691)\n","\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:130)\n","\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:298)\n","\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:551)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:550)\n","\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq3%2Ftest_emit.csv-1651605240000?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 58 more\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/03 19:14:10 WARN org.apache.spark.streaming.dstream.FileInputDStream: Error finding new files under gs://genishk-bucket1/testq\n","java.io.IOException: Error accessing gs://genishk-bucket1/testq\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)\n","\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:281)\n","\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\n","\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2069)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1098)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1059)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:195)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:146)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n","\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n","\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n","\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n","\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n","\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n","\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n","\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 42 more\n","22/05/03 19:14:10 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651605250000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 262, in saveAsTextFile\n","    rdd.saveAsTextFile(path)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1826, in saveAsTextFile\n","    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o129.saveAsTextFile.\n",": java.io.IOException: Error accessing gs://genishk-bucket1/testq3/test_emit.csv-1651605250000\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1691)\n","\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:130)\n","\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:298)\n","\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:551)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:550)\n","\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq3%2Ftest_emit.csv-1651605250000?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 58 more\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/03 19:14:20 WARN org.apache.spark.streaming.dstream.FileInputDStream: Error finding new files under gs://genishk-bucket1/testq\n","java.io.IOException: Error accessing gs://genishk-bucket1/testq\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)\n","\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:281)\n","\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\n","\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2069)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1098)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1059)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:195)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:146)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n","\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n","\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n","\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n","\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n","\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n","\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n","\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 42 more\n","22/05/03 19:14:20 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651605260000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 262, in saveAsTextFile\n","    rdd.saveAsTextFile(path)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1826, in saveAsTextFile\n","    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o144.saveAsTextFile.\n",": java.io.IOException: Error accessing gs://genishk-bucket1/testq3/test_emit.csv-1651605260000\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1691)\n","\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:130)\n","\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:298)\n","\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:551)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:550)\n","\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq3%2Ftest_emit.csv-1651605260000?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 58 more\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/03 19:14:30 WARN org.apache.spark.streaming.dstream.FileInputDStream: Error finding new files under gs://genishk-bucket1/testq\n","java.io.IOException: Error accessing gs://genishk-bucket1/testq\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)\n","\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:281)\n","\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\n","\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2069)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1098)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1059)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:195)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:146)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n","\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n","\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n","\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n","\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n","\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n","\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n","\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 42 more\n","22/05/03 19:14:30 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651605270000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 262, in saveAsTextFile\n","    rdd.saveAsTextFile(path)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1826, in saveAsTextFile\n","    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o159.saveAsTextFile.\n",": java.io.IOException: Error accessing gs://genishk-bucket1/testq3/test_emit.csv-1651605270000\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1691)\n","\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:130)\n","\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:298)\n","\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:551)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:550)\n","\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq3%2Ftest_emit.csv-1651605270000?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 58 more\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/03 19:14:40 WARN org.apache.spark.streaming.dstream.FileInputDStream: Error finding new files under gs://genishk-bucket1/testq\n","java.io.IOException: Error accessing gs://genishk-bucket1/testq\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)\n","\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:281)\n","\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\n","\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2069)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1098)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1059)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:195)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:146)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n","\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n","\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n","\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n","\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n","\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n","\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n","\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 42 more\n","22/05/03 19:14:40 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651605280000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 262, in saveAsTextFile\n","    rdd.saveAsTextFile(path)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1826, in saveAsTextFile\n","    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o174.saveAsTextFile.\n",": java.io.IOException: Error accessing gs://genishk-bucket1/testq3/test_emit.csv-1651605280000\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1691)\n","\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:130)\n","\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:298)\n","\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:551)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:550)\n","\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq3%2Ftest_emit.csv-1651605280000?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 58 more\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/03 19:14:50 WARN org.apache.spark.streaming.dstream.FileInputDStream: Error finding new files under gs://genishk-bucket1/testq\n","java.io.IOException: Error accessing gs://genishk-bucket1/testq\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)\n","\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:281)\n","\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\n","\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2069)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1098)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1059)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:195)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:146)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n","\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n","\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n","\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n","\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n","\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n","\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n","\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 42 more\n","22/05/03 19:14:50 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651605290000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 262, in saveAsTextFile\n","    rdd.saveAsTextFile(path)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1826, in saveAsTextFile\n","    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o189.saveAsTextFile.\n",": java.io.IOException: Error accessing gs://genishk-bucket1/testq3/test_emit.csv-1651605290000\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1691)\n","\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:130)\n","\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:298)\n","\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:551)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:550)\n","\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq3%2Ftest_emit.csv-1651605290000?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 58 more\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/03 19:15:00 WARN org.apache.spark.streaming.dstream.FileInputDStream: Error finding new files under gs://genishk-bucket1/testq\n","java.io.IOException: Error accessing gs://genishk-bucket1/testq\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)\n","\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:281)\n","\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\n","\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2069)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1098)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1059)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:195)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:146)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n","\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n","\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n","\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n","\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n","\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n","\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n","\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 42 more\n","22/05/03 19:15:00 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651605300000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 262, in saveAsTextFile\n","    rdd.saveAsTextFile(path)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1826, in saveAsTextFile\n","    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o204.saveAsTextFile.\n",": java.io.IOException: Error accessing gs://genishk-bucket1/testq3/test_emit.csv-1651605300000\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2175)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2062)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n","\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n","\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1691)\n","\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:130)\n","\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:298)\n","\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n","\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n","\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:551)\n","\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:550)\n","\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n","GET https://storage.googleapis.com/storage/v1/b/genishk-bucket1/o/testq3%2Ftest_emit.csv-1651605300000?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n","{\n","  \"code\" : 403,\n","  \"errors\" : [ {\n","    \"domain\" : \"global\",\n","    \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n","    \"reason\" : \"forbidden\"\n","  } ],\n","  \"message\" : \"1058489087692-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n","}\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n","\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2169)\n","\t... 58 more\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n"]}],"source":["ssc.start()"]},{"cell_type":"code","execution_count":null,"id":"9fb401f3","metadata":{},"outputs":[],"source":["ssc.awaitTermination()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}